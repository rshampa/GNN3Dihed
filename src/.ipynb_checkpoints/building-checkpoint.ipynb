{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from folder\n",
      "Initializing Molecular Representation Generator\n",
      "Dataset Name: LogP\n",
      "Number of Molecules Loaded: 4197\n"
     ]
    }
   ],
   "source": [
    "from model import Autoencoder, GNN3D\n",
    "from dataset import LogPDataset\n",
    "import torch\n",
    "\n",
    "# Loading LogP dataset\n",
    "dataset = LogPDataset(\"../data/logp\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_autoencoder = Autoencoder(80, 10).to(device)\n",
    "bond_autoencoder = Autoencoder(10, 3).to(device)\n",
    "\n",
    "mse_loss_fn = torch.nn.MSELoss()\n",
    "atom_autoencoder_optimizer = torch.optim.Adam(atom_autoencoder.parameters())\n",
    "bond_autoencoder_optimizer = torch.optim.Adam(bond_autoencoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "printstep = 500\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for epoch_i in range(n_epochs):\n",
    "#     avg_atom_rmse = 0\n",
    "#     avg_bond_rmse = 0\n",
    "#     for i, molecule in enumerate(dataset):\n",
    "#         atomic_vectors = molecule[0].to(device)\n",
    "#         bond_vectors = molecule[1].to(device)\n",
    "        \n",
    "#         atomic_vector_reconstruction = atom_autoencoder(atomic_vectors)\n",
    "#         bond_vector_reconstruction = bond_autoencoder(bond_vectors)\n",
    "    \n",
    "#         # Computing losses\n",
    "#         atomic_reconstruction_loss = mse_loss_fn(atomic_vectors, atomic_vector_reconstruction)\n",
    "#         bond_reconstruction_loss = mse_loss_fn(bond_vectors, bond_vector_reconstruction)\n",
    "    \n",
    "#         # Taking optimization step\n",
    "#         atom_autoencoder_optimizer.zero_grad()\n",
    "#         bond_autoencoder_optimizer.zero_grad()\n",
    "    \n",
    "#         atomic_reconstruction_loss.backward()\n",
    "#         bond_reconstruction_loss.backward()\n",
    "    \n",
    "#         atom_autoencoder_optimizer.step()\n",
    "#         bond_autoencoder_optimizer.step()\n",
    "    \n",
    "#         # Updating average losses\n",
    "#         avg_atom_rmse = (avg_atom_rmse * i + atomic_reconstruction_loss.item() ** 0.5) / (i + 1)\n",
    "#         avg_bond_rmse = (avg_bond_rmse * i + bond_reconstruction_loss.item() ** 0.5) / (i + 1)\n",
    "    \n",
    "#         if (i % printstep == 0):\n",
    "#             print(f\"Epoch. {epoch_i}, Example. {i}, avg atom rmse: {avg_atom_rmse}, avg bond rmse: {avg_bond_rmse}\")\n",
    "\n",
    "# # Saving models somewhere\n",
    "# torch.save(atom_autoencoder.state_dict(), \"./models/logp_atom_autoencoder.pth\")\n",
    "# torch.save(bond_autoencoder.state_dict(), \"./models/logp_bond_autoencoder.pth\")\n",
    "\n",
    "atom_autoencoder.load_state_dict(torch.load(\"./models/logp_atom_autoencoder.pth\"))\n",
    "bond_autoencoder.load_state_dict(torch.load(\"./models/logp_bond_autoencoder.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Atomic Vector:\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "\n",
      "Reconstruction:\n",
      "tensor([[ 9.9633e-01,  3.5745e-03, -1.0154e-03,  ...,  7.7394e-04,\n",
      "          5.7995e-04, -2.0181e-03],\n",
      "        [ 2.9183e-02,  9.8448e-01,  3.5630e-03,  ...,  2.7819e-04,\n",
      "         -1.9600e-03,  1.0222e-03],\n",
      "        [ 1.0032e+00, -6.7498e-03,  1.2391e-04,  ..., -4.2486e-04,\n",
      "         -1.3446e-03, -8.2866e-05],\n",
      "        ...,\n",
      "        [-1.2934e-03, -2.5709e-03, -2.2012e-04,  ...,  3.7370e-04,\n",
      "         -3.3575e-04,  9.6865e-05],\n",
      "        [-1.2934e-03, -2.5709e-03, -2.2012e-04,  ...,  3.7370e-04,\n",
      "         -3.3575e-04,  9.6865e-05],\n",
      "        [-1.2934e-03, -2.5709e-03, -2.2012e-04,  ...,  3.7370e-04,\n",
      "         -3.3575e-04,  9.6865e-05]], device='cuda:0', grad_fn=<GeluBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Test Bond Vector:\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], device='cuda:0')\n",
      "\n",
      "Reconstruction:\n",
      "tensor([[ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0013e+00, -2.0254e-04, -1.7550e-03,  1.1281e-04, -6.7200e-04,\n",
      "          9.9761e-01, -1.3778e-04,  4.9222e-05, -4.2637e-04,  1.0014e+00],\n",
      "        [ 1.0013e+00, -2.0254e-04, -1.7550e-03,  1.1281e-04, -6.7200e-04,\n",
      "          9.9761e-01, -1.3778e-04,  4.9222e-05, -4.2637e-04,  1.0014e+00],\n",
      "        [ 1.0013e+00, -2.0254e-04, -1.7550e-03,  1.1281e-04, -6.7200e-04,\n",
      "          9.9761e-01, -1.3778e-04,  4.9222e-05, -4.2637e-04,  1.0014e+00],\n",
      "        [ 1.0013e+00, -2.0254e-04, -1.7550e-03,  1.1281e-04, -6.7200e-04,\n",
      "          9.9761e-01, -1.3778e-04,  4.9222e-05, -4.2637e-04,  1.0014e+00],\n",
      "        [ 1.0013e+00, -2.0254e-04, -1.7550e-03,  1.1281e-04, -6.7200e-04,\n",
      "          9.9761e-01, -1.3778e-04,  4.9222e-05, -4.2637e-04,  1.0014e+00],\n",
      "        [ 9.9918e-01, -1.8468e-04,  1.0345e-03,  2.2359e-03,  9.9822e-01,\n",
      "          1.0063e-03,  8.1013e-05, -1.0873e-04, -1.0096e-03,  1.0010e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [ 1.0013e+00, -2.0254e-04, -1.7550e-03,  1.1281e-04, -6.7200e-04,\n",
      "          9.9761e-01, -1.3778e-04,  4.9222e-05, -4.2637e-04,  1.0014e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0013e+00, -2.0254e-04, -1.7550e-03,  1.1281e-04, -6.7200e-04,\n",
      "          9.9761e-01, -1.3778e-04,  4.9222e-05, -4.2637e-04,  1.0014e+00],\n",
      "        [ 1.0013e+00, -2.0254e-04, -1.7550e-03,  1.1281e-04, -6.7200e-04,\n",
      "          9.9761e-01, -1.3778e-04,  4.9222e-05, -4.2637e-04,  1.0014e+00],\n",
      "        [ 1.0013e+00, -2.0254e-04, -1.7550e-03,  1.1281e-04, -6.7200e-04,\n",
      "          9.9761e-01, -1.3778e-04,  4.9222e-05, -4.2637e-04,  1.0014e+00],\n",
      "        [ 1.0013e+00, -2.0254e-04, -1.7550e-03,  1.1281e-04, -6.7200e-04,\n",
      "          9.9761e-01, -1.3778e-04,  4.9222e-05, -4.2637e-04,  1.0014e+00],\n",
      "        [ 1.0013e+00, -2.0254e-04, -1.7550e-03,  1.1281e-04, -6.7200e-04,\n",
      "          9.9761e-01, -1.3778e-04,  4.9222e-05, -4.2637e-04,  1.0014e+00],\n",
      "        [ 9.9918e-01, -1.8468e-04,  1.0345e-03,  2.2359e-03,  9.9822e-01,\n",
      "          1.0063e-03,  8.1013e-05, -1.0873e-04, -1.0096e-03,  1.0010e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [ 1.0013e+00, -2.0254e-04, -1.7550e-03,  1.1281e-04, -6.7200e-04,\n",
      "          9.9761e-01, -1.3778e-04,  4.9222e-05, -4.2637e-04,  1.0014e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [-9.3762e-04,  2.1363e-04,  3.5676e-04,  9.9944e-01,  9.9932e-01,\n",
      "          1.0006e+00,  3.0987e-04,  6.2585e-04, -2.0874e-08,  1.0007e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00],\n",
      "        [ 1.0010e+00, -3.3152e-04, -4.3243e-04,  4.0921e-05,  2.0819e-04,\n",
      "         -4.5439e-04, -4.7300e-05,  6.8397e-06,  3.3827e-04,  1.0011e+00]],\n",
      "       device='cuda:0', grad_fn=<GeluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing atomic and bond reconstruction\n",
    "\n",
    "test_i = 0\n",
    "test_atomic_vectors = dataset[test_i][0].to(device)\n",
    "test_bond_vectors = dataset[test_i][1].to(device)\n",
    "reconstructed_atomic_vectors = atom_autoencoder(test_atomic_vectors)\n",
    "reconstructed_bond_vectors = bond_autoencoder(test_bond_vectors)\n",
    "\n",
    "print(f\"Test Atomic Vector:\\n{test_atomic_vectors}\\n\\nReconstruction:\\n{reconstructed_atomic_vectors}\")\n",
    "print(f\"\\n\\n\\nTest Bond Vector:\\n{test_bond_vectors}\\n\\nReconstruction:\\n{reconstructed_bond_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting to run Vanilla GEM Implementation (to test reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from folder\n",
      "Initializing Molecular Representation Generator\n",
      "Dataset Name: LogP\n",
      "Number of Molecules Loaded: 4197\n"
     ]
    }
   ],
   "source": [
    "from model import Autoencoder, GNN3D\n",
    "from dataset import LogPDataset\n",
    "import torch\n",
    "\n",
    "# Loading LogP dataset\n",
    "dataset = LogPDataset(\"../data/logp\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verification\n",
    "\n",
    "dataset[4196][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN3D(\n",
       "  (atom_bond_operator): DMPNNLayer(\n",
       "    (message_generation_network): Sequential(\n",
       "      (0): Linear(in_features=170, out_features=85, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=85, out_features=80, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "    )\n",
       "    (combination_network): Sequential(\n",
       "      (0): Linear(in_features=160, out_features=120, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=120, out_features=80, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (bond_angle_operator): DMPNNLayer(\n",
       "    (message_generation_network): Sequential(\n",
       "      (0): Linear(in_features=21, out_features=10, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "    )\n",
       "    (combination_network): Sequential(\n",
       "      (0): Linear(in_features=20, out_features=15, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=15, out_features=10, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (angle_dihedral_operator): DMPNNLayer(\n",
       "    (message_generation_network): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=1, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "    )\n",
       "    (combination_network): Sequential(\n",
       "      (0): Linear(in_features=2, out_features=1, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (readout): Sequential(\n",
       "    (0): Linear(in_features=280, out_features=30, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading GNN3D\n",
    "from model import GNN3D\n",
    "\n",
    "# Making an instance of the model and an optimizer\n",
    "gnn3d = GNN3D(atomic_vector_size= 80, bond_vector_size=10, number_of_molecular_features = 200, number_of_targets = 1).to(device)\n",
    "gnn3d_optimizer = torch.optim.Adam(gnn3d.parameters())\n",
    "gnn3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "printstep = 20\n",
    "n_epochs = 50\n",
    "avg_rmse_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 0/50, Ex. 0, avg rmse: 0.19368982315063477, rolling rmse: 0.19368982315063477, immediate mse: 0.03751574829220772, target: 3.5399999618530273, pred: 3.733689785003662\n",
      "Ep. 0/50, Ex. 20, avg rmse: 0.7364351451396942, rolling rmse: 0.7635724112391472, immediate mse: 0.6645601391792297, target: 2.9200000762939453, pred: 2.104794502258301\n",
      "Ep. 0/50, Ex. 40, avg rmse: 0.7312813030510414, rolling rmse: 0.725869768857956, immediate mse: 0.2599448561668396, target: 1.399999976158142, pred: 1.9098478555679321\n",
      "Ep. 0/50, Ex. 60, avg rmse: 0.7553738595032301, rolling rmse: 0.804763600230217, immediate mse: 1.56436026096344, target: 1.190000057220459, pred: 2.440743923187256\n",
      "Ep. 0/50, Ex. 80, avg rmse: 0.6795351760990826, rolling rmse: 0.44822719171643255, immediate mse: 0.03570973873138428, target: 2.0, pred: 1.8110297918319702\n",
      "Ep. 0/50, Ex. 100, avg rmse: 0.6748089189871702, rolling rmse: 0.6556675776839256, immediate mse: 0.020617395639419556, target: 1.4900000095367432, pred: 1.3464124202728271\n",
      "Ep. 0/50, Ex. 120, avg rmse: 0.662297813483506, rolling rmse: 0.5991167306900025, immediate mse: 0.9040717482566833, target: 3.5999999046325684, pred: 2.6491730213165283\n",
      "Ep. 0/50, Ex. 140, avg rmse: 0.6575916306980958, rolling rmse: 0.6291192248463631, immediate mse: 0.0706767663359642, target: 3.6700000762939453, pred: 3.404149055480957\n",
      "Ep. 0/50, Ex. 160, avg rmse: 0.6640424365582676, rolling rmse: 0.7095206178724767, immediate mse: 1.467732310295105, target: -1.340000033378601, pred: -0.12850001454353333\n",
      "Ep. 0/50, Ex. 180, avg rmse: 0.6575621281704195, rolling rmse: 0.6053956456482411, immediate mse: 0.5755017995834351, target: 2.200000047683716, pred: 2.958618402481079\n",
      "Ep. 0/50, Ex. 200, avg rmse: 0.6540575876313065, rolling rmse: 0.6223414957523346, immediate mse: 0.24181877076625824, target: 2.359999895095825, pred: 2.851750612258911\n",
      "Ep. 0/50, Ex. 220, avg rmse: 0.6645592242749031, rolling rmse: 0.7701006725430488, immediate mse: 0.07922861725091934, target: 2.309999942779541, pred: 2.59147572517395\n",
      "Ep. 0/50, Ex. 240, avg rmse: 0.6678768920082276, rolling rmse: 0.7045371204614639, immediate mse: 0.37705788016319275, target: 0.75, pred: 1.3640503883361816\n",
      "Ep. 0/50, Ex. 260, avg rmse: 0.6712808505671236, rolling rmse: 0.7122985512018204, immediate mse: 0.33936434984207153, target: 0.4000000059604645, pred: -0.1825498789548874\n",
      "Ep. 0/50, Ex. 280, avg rmse: 0.6699694025452878, rolling rmse: 0.6528550058603286, immediate mse: 0.011595616117119789, target: 2.799999952316284, pred: 2.692317008972168\n",
      "Ep. 0/50, Ex. 300, avg rmse: 0.6864600630594647, rolling rmse: 0.9181538432836532, immediate mse: 0.9628217220306396, target: 1.9199999570846558, pred: 0.9387651681900024\n",
      "Ep. 0/50, Ex. 320, avg rmse: 0.6903518001517032, rolling rmse: 0.7489224433898926, immediate mse: 0.04391825571656227, target: 3.200000047683716, pred: 3.409566879272461\n",
      "Ep. 0/50, Ex. 340, avg rmse: 0.6778047463673655, rolling rmse: 0.4764245331287384, immediate mse: 0.03609177842736244, target: 2.9600000381469727, pred: 2.770021677017212\n",
      "Ep. 0/50, Ex. 360, avg rmse: 0.6781148322318733, rolling rmse: 0.6834017962217331, immediate mse: 0.8146166801452637, target: 3.0199999809265137, pred: 2.117438793182373\n",
      "Ep. 0/50, Ex. 380, avg rmse: 0.6910559770863828, rolling rmse: 0.9246436417102813, immediate mse: 2.4956440925598145, target: 0.9900000095367432, pred: 2.569760799407959\n",
      "Ep. 0/50, Ex. 400, avg rmse: 0.6898903225053874, rolling rmse: 0.6676846027374268, immediate mse: 0.012615717016160488, target: 1.3300000429153442, pred: 1.4423197507858276\n",
      "Ep. 0/50, Ex. 420, avg rmse: 0.6838117133905657, rolling rmse: 0.5619356006383895, immediate mse: 0.8310139179229736, target: 2.4100000858306885, pred: 3.3215997219085693\n",
      "Ep. 0/50, Ex. 440, avg rmse: 0.6899510609522429, rolling rmse: 0.8191843271255493, immediate mse: 1.5320706367492676, target: -0.2800000011920929, pred: 0.9577683806419373\n",
      "Ep. 0/50, Ex. 460, avg rmse: 0.690598937894931, rolling rmse: 0.7048846244812011, immediate mse: 1.0022751092910767, target: 1.840000033378601, pred: 2.841136932373047\n",
      "Ep. 0/50, Ex. 480, avg rmse: 0.6919484293076701, rolling rmse: 0.7230542063713074, immediate mse: 2.7394285202026367, target: 1.600000023841858, pred: 3.255121946334839\n",
      "Ep. 0/50, Ex. 500, avg rmse: 0.6932049807376729, rolling rmse: 0.7234250426292419, immediate mse: 2.692209005355835, target: 1.2100000381469727, pred: 2.850795269012451\n",
      "Ep. 0/50, Ex. 520, avg rmse: 0.6922249643850694, rolling rmse: 0.6676755547523499, immediate mse: 0.0880499929189682, target: 2.2100000381469727, pred: 2.506732225418091\n",
      "Ep. 0/50, Ex. 540, avg rmse: 0.6857426697302218, rolling rmse: 0.5168788939714432, immediate mse: 0.8741869926452637, target: 3.700000047683716, pred: 2.7650203704833984\n",
      "Ep. 0/50, Ex. 560, avg rmse: 0.6836388456152085, rolling rmse: 0.6267304033041, immediate mse: 0.0012780221877619624, target: 1.399999976158142, pred: 1.4357494115829468\n",
      "Ep. 0/50, Ex. 580, avg rmse: 0.6841430831027319, rolling rmse: 0.6982869446277618, immediate mse: 0.8466565608978271, target: 1.6100000143051147, pred: 2.530139446258545\n",
      "Ep. 0/50, Ex. 600, avg rmse: 0.6783546059331957, rolling rmse: 0.5101993441581726, immediate mse: 0.01516037154942751, target: 2.0, pred: 2.1231274604797363\n",
      "Ep. 0/50, Ex. 620, avg rmse: 0.6777195426337002, rolling rmse: 0.6586358904838562, immediate mse: 0.7487720251083374, target: 3.4100000858306885, pred: 2.5446839332580566\n",
      "Ep. 0/50, Ex. 640, avg rmse: 0.6813991371768875, rolling rmse: 0.7956505477428436, immediate mse: 0.7814194560050964, target: 0.5099999904632568, pred: 1.3939793109893799\n",
      "Ep. 0/50, Ex. 660, avg rmse: 0.6762316134756888, rolling rmse: 0.510612478852272, immediate mse: 0.16302745044231415, target: 0.7900000214576721, pred: 1.1937665939331055\n",
      "Ep. 0/50, Ex. 680, avg rmse: 0.6733748196636533, rolling rmse: 0.5789577841758727, immediate mse: 0.13611610233783722, target: 1.9900000095367432, pred: 2.3589391708374023\n",
      "Ep. 0/50, Ex. 700, avg rmse: 0.675750516939606, rolling rmse: 0.756643009185791, immediate mse: 1.9201868772506714, target: -0.5899999737739563, pred: 0.7957080602645874\n",
      "Ep. 0/50, Ex. 720, avg rmse: 0.6776339411528868, rolling rmse: 0.7436479598283767, immediate mse: 0.6479579210281372, target: 2.9000000953674316, pred: 2.0950417518615723\n",
      "Ep. 0/50, Ex. 740, avg rmse: 0.6760337713718746, rolling rmse: 0.6183476507663727, immediate mse: 1.3038513660430908, target: 0.2199999988079071, pred: 1.361863136291504\n",
      "Ep. 0/50, Ex. 760, avg rmse: 0.6753328425929687, rolling rmse: 0.6493634313344956, immediate mse: 0.7452282905578613, target: 0.11999999731779099, pred: 0.9832660555839539\n",
      "Ep. 0/50, Ex. 780, avg rmse: 0.6698215952412584, rolling rmse: 0.46011863350868226, immediate mse: 0.05838267505168915, target: 3.200000047683716, pred: 3.4416251182556152\n",
      "Ep. 0/50, Ex. 800, avg rmse: 0.6735610960983914, rolling rmse: 0.8195886045694352, immediate mse: 0.26286113262176514, target: 2.4100000858306885, pred: 2.9226999282836914\n",
      "Ep. 0/50, Ex. 820, avg rmse: 0.6758775335535799, rolling rmse: 0.7686508536338806, immediate mse: 0.5871579051017761, target: 2.7300000190734863, pred: 1.9637377262115479\n",
      "Ep. 0/50, Ex. 840, avg rmse: 0.6760481761659946, rolling rmse: 0.6830530554056167, immediate mse: 1.0036256313323975, target: 1.309999942779541, pred: 0.308188796043396\n",
      "Ep. 0/50, Ex. 860, avg rmse: 0.6741918719846346, rolling rmse: 0.5961342811584472, immediate mse: 2.378148078918457, target: 3.799999952316284, pred: 2.257875442504883\n",
      "Ep. 0/50, Ex. 880, avg rmse: 0.6741170885715809, rolling rmse: 0.6708976626396179, immediate mse: 0.052574727684259415, target: 1.7100000381469727, pred: 1.9392918348312378\n",
      "Ep. 0/50, Ex. 900, avg rmse: 0.6783604735506504, rolling rmse: 0.8652815818786621, immediate mse: 0.5889529585838318, target: 2.819999933242798, pred: 2.05256724357605\n",
      "Ep. 0/50, Ex. 920, avg rmse: 0.6830646923614775, rolling rmse: 0.894989749789238, immediate mse: 0.02463049441576004, target: 1.3799999952316284, pred: 1.5369410514831543\n",
      "Ep. 0/50, Ex. 940, avg rmse: 0.6825573200342135, rolling rmse: 0.6591928243637085, immediate mse: 0.041853051632642746, target: 1.7300000190734863, pred: 1.52541983127594\n",
      "Ep. 0/50, Ex. 960, avg rmse: 0.6784918816075994, rolling rmse: 0.4872130036354064, immediate mse: 0.0057267071679234505, target: 1.600000023841858, pred: 1.5243250131607056\n",
      "Ep. 0/50, Ex. 980, avg rmse: 0.6804479398099178, rolling rmse: 0.7744365364313126, immediate mse: 0.28319013118743896, target: 1.5800000429153442, pred: 2.1121561527252197\n",
      "Ep. 0/50, Ex. 1000, avg rmse: 0.6789894708833265, rolling rmse: 0.607451570034027, immediate mse: 0.611501932144165, target: 1.6100000143051147, pred: 2.3919858932495117\n",
      "Ep. 0/50, Ex. 1020, avg rmse: 0.6812377839288104, rolling rmse: 0.793765851855278, immediate mse: 0.6000815629959106, target: -0.10000000149011612, pred: 0.6746492981910706\n",
      "Ep. 0/50, Ex. 1040, avg rmse: 0.684507950854119, rolling rmse: 0.8514499723911285, immediate mse: 0.19268155097961426, target: 2.5, pred: 2.061044931411743\n",
      "Ep. 0/50, Ex. 1060, avg rmse: 0.6846829807696543, rolling rmse: 0.6937932878732682, immediate mse: 0.34212589263916016, target: 1.600000023841858, pred: 2.18491530418396\n",
      "Ep. 0/50, Ex. 1080, avg rmse: 0.6861223843806774, rolling rmse: 0.7624827459454537, immediate mse: 0.09355722367763519, target: 2.700000047683716, pred: 2.3941287994384766\n",
      "Ep. 0/50, Ex. 1100, avg rmse: 0.68304226640189, rolling rmse: 0.5165618896484375, immediate mse: 0.16042909026145935, target: 0.6700000166893005, pred: 1.0705360174179077\n",
      "Ep. 0/50, Ex. 1120, avg rmse: 0.6834299890698338, rolling rmse: 0.704774121940136, immediate mse: 0.010294175706803799, target: 2.7100000381469727, pred: 2.811460256576538\n",
      "Ep. 0/50, Ex. 1140, avg rmse: 0.6807513316533322, rolling rmse: 0.5306125834584235, immediate mse: 0.6946946978569031, target: 4.230000019073486, pred: 3.3965165615081787\n",
      "Ep. 0/50, Ex. 1160, avg rmse: 0.6800543102311438, rolling rmse: 0.6402892380952835, immediate mse: 0.09892500936985016, target: 2.1600000858306885, pred: 2.4745235443115234\n",
      "Ep. 0/50, Ex. 1180, avg rmse: 0.6784729891591518, rolling rmse: 0.5866773009300232, immediate mse: 0.2116941511631012, target: 2.200000047683716, pred: 2.660102367401123\n",
      "Ep. 0/50, Ex. 1200, avg rmse: 0.6753445436664668, rolling rmse: 0.4906098373234272, immediate mse: 1.959781289100647, target: 1.2999999523162842, pred: 2.6999218463897705\n",
      "Ep. 0/50, Ex. 1220, avg rmse: 0.6724753848213727, rolling rmse: 0.5001823961734772, immediate mse: 1.6070129871368408, target: -0.5, pred: 0.7676801085472107\n",
      "Ep. 0/50, Ex. 1240, avg rmse: 0.6758757953582324, rolling rmse: 0.8834708586335183, immediate mse: 0.34826648235321045, target: 3.069999933242798, pred: 2.479858875274658\n",
      "Ep. 0/50, Ex. 1260, avg rmse: 0.6752994100693956, rolling rmse: 0.6395347028970718, immediate mse: 0.018189480528235435, target: 2.5199999809265137, pred: 2.385131597518921\n",
      "Ep. 0/50, Ex. 1280, avg rmse: 0.6750592116752145, rolling rmse: 0.6599147029221059, immediate mse: 0.006962254643440247, target: 0.8999999761581421, pred: 0.983440101146698\n",
      "Ep. 0/50, Ex. 1300, avg rmse: 0.6717167658062734, rolling rmse: 0.4576331079006195, immediate mse: 0.10396488010883331, target: 2.0299999713897705, pred: 1.707564115524292\n",
      "Ep. 0/50, Ex. 1320, avg rmse: 0.670383918395916, rolling rmse: 0.58368219435215, immediate mse: 0.3793049454689026, target: 3.75, pred: 3.134122610092163\n",
      "Ep. 0/50, Ex. 1340, avg rmse: 0.6683595011276067, rolling rmse: 0.5346467405557632, immediate mse: 0.22884847223758698, target: 0.9200000166893005, pred: 0.4416189193725586\n",
      "Ep. 0/50, Ex. 1360, avg rmse: 0.6663873877203879, rolling rmse: 0.534157183766365, immediate mse: 0.31428202986717224, target: 2.0899999141693115, pred: 2.650608539581299\n",
      "Ep. 0/50, Ex. 1380, avg rmse: 0.6649970637228421, rolling rmse: 0.57038551568985, immediate mse: 0.07834124565124512, target: 2.6500000953674316, pred: 2.9298951625823975\n",
      "Ep. 0/50, Ex. 1400, avg rmse: 0.6654661968403598, rolling rmse: 0.6978598386049271, immediate mse: 0.23259907960891724, target: 2.8399999141693115, pred: 2.3577146530151367\n",
      "Ep. 0/50, Ex. 1420, avg rmse: 0.6649359813737338, rolling rmse: 0.6277943879365921, immediate mse: 0.4021463394165039, target: 0.20000000298023224, pred: 0.8341500759124756\n",
      "Ep. 0/50, Ex. 1440, avg rmse: 0.6682986834349528, rolling rmse: 0.9072186648845673, immediate mse: 0.9199725389480591, target: 3.0, pred: 2.0408480167388916\n",
      "Ep. 0/50, Ex. 1460, avg rmse: 0.6675032815914465, rolling rmse: 0.6101945787668228, immediate mse: 0.38280802965164185, target: 2.809999942779541, pred: 2.1912851333618164\n",
      "Ep. 0/50, Ex. 1480, avg rmse: 0.6685262817152446, rolling rmse: 0.7432564407587052, immediate mse: 1.6796883344650269, target: 2.799999952316284, pred: 1.503972053527832\n",
      "Ep. 0/50, Ex. 1500, avg rmse: 0.669610931595749, rolling rmse: 0.7499292552471161, immediate mse: 0.012734250165522099, target: 3.259999990463257, pred: 3.147153854370117\n",
      "Ep. 0/50, Ex. 1520, avg rmse: 0.6701236721581334, rolling rmse: 0.7086048513650894, immediate mse: 0.18598672747612, target: 1.690000057220459, pred: 2.1212618350982666\n",
      "Ep. 0/50, Ex. 1540, avg rmse: 0.6709807531486309, rolling rmse: 0.7361617624759674, immediate mse: 0.04331199452280998, target: 2.5899999141693115, pred: 2.7981152534484863\n",
      "Ep. 0/50, Ex. 1560, avg rmse: 0.6705107564902474, rolling rmse: 0.6342975139617921, immediate mse: 0.22970199584960938, target: 1.1799999475479126, pred: 1.659272313117981\n",
      "Ep. 0/50, Ex. 1580, avg rmse: 0.6701350711347331, rolling rmse: 0.6408128291368484, immediate mse: 0.6162083148956299, target: 0.7200000286102295, pred: -0.06498933583498001\n",
      "Ep. 0/50, Ex. 1600, avg rmse: 0.6719090619603222, rolling rmse: 0.8121430367231369, immediate mse: 8.60814380645752, target: -0.8500000238418579, pred: 2.083963632583618\n",
      "Ep. 0/50, Ex. 1620, avg rmse: 0.6730609456839053, rolling rmse: 0.7652692377567292, immediate mse: 0.08422987908124924, target: 3.4200000762939453, pred: 3.1297762393951416\n",
      "Ep. 0/50, Ex. 1640, avg rmse: 0.6726589796475562, rolling rmse: 0.6400796324014664, immediate mse: 0.018913935869932175, target: 3.3499999046325684, pred: 3.487527847290039\n",
      "Ep. 0/50, Ex. 1660, avg rmse: 0.6725731043572314, rolling rmse: 0.6655270367860794, immediate mse: 0.44392526149749756, target: 2.799999952316284, pred: 2.1337227821350098\n",
      "Ep. 0/50, Ex. 1680, avg rmse: 0.6726162576076178, rolling rmse: 0.6762001350522041, immediate mse: 0.0004922733060084283, target: 0.8999999761581421, pred: 0.9221872091293335\n",
      "Ep. 0/50, Ex. 1700, avg rmse: 0.6765250256925663, rolling rmse: 1.005056983232498, immediate mse: 0.6158048510551453, target: -0.33000001311302185, pred: 0.45473232865333557\n",
      "Ep. 0/50, Ex. 1720, avg rmse: 0.6763463156175502, rolling rmse: 0.6611470237374306, immediate mse: 0.03361174464225769, target: 3.680000066757202, pred: 3.4966650009155273\n",
      "Ep. 0/50, Ex. 1740, avg rmse: 0.6758269268520806, rolling rmse: 0.6311335235834121, immediate mse: 0.03715751692652702, target: 2.630000114440918, pred: 2.43723726272583\n",
      "Ep. 0/50, Ex. 1760, avg rmse: 0.6745858481582222, rolling rmse: 0.5665499478578567, immediate mse: 0.7050792574882507, target: 3.880000114440918, pred: 3.0403101444244385\n",
      "Ep. 0/50, Ex. 1780, avg rmse: 0.6727197230247083, rolling rmse: 0.5084074050188063, immediate mse: 0.3063344657421112, target: 0.6000000238418579, pred: 1.1534749269485474\n",
      "Ep. 0/50, Ex. 1800, avg rmse: 0.6727123782493749, rolling rmse: 0.6720583260059356, immediate mse: 0.010426384396851063, target: 1.2599999904632568, pred: 1.1578903198242188\n",
      "Ep. 0/50, Ex. 1820, avg rmse: 0.6740609862642718, rolling rmse: 0.7955031380057336, immediate mse: 1.6936756372451782, target: 1.9900000095367432, pred: 0.6885870695114136\n",
      "Ep. 0/50, Ex. 1840, avg rmse: 0.673097004123449, rolling rmse: 0.5853264302015304, immediate mse: 0.06389792263507843, target: 0.8399999737739563, pred: 1.092780351638794\n",
      "Ep. 0/50, Ex. 1860, avg rmse: 0.6741869489891051, rolling rmse: 0.774516373872757, immediate mse: 3.882502794265747, target: -0.3700000047683716, pred: 1.6004067659378052\n",
      "Ep. 0/50, Ex. 1880, avg rmse: 0.6736366576428492, rolling rmse: 0.6224320478737354, immediate mse: 0.0023986431770026684, target: 1.8799999952316284, pred: 1.9289759397506714\n",
      "Ep. 0/50, Ex. 1900, avg rmse: 0.6729201235343506, rolling rmse: 0.6055300906300545, immediate mse: 0.017872553318738937, target: -0.8999999761581421, pred: -0.7663117051124573\n",
      "Ep. 0/50, Ex. 1920, avg rmse: 0.673985059685784, rolling rmse: 0.7752072408795356, immediate mse: 0.6022115349769592, target: 3.25, pred: 4.026022911071777\n",
      "Ep. 0/50, Ex. 1940, avg rmse: 0.6734056889565565, rolling rmse: 0.6177571304142475, immediate mse: 0.74897301197052, target: 3.700000047683716, pred: 2.8345677852630615\n",
      "Ep. 0/50, Ex. 1960, avg rmse: 0.673876734312732, rolling rmse: 0.71959168612957, immediate mse: 3.894423246383667, target: 0.6000000238418579, pred: 2.5734293460845947\n",
      "Ep. 0/50, Ex. 1980, avg rmse: 0.6730795413075162, rolling rmse: 0.5949147671461106, immediate mse: 0.07493981719017029, target: 1.9199999570846558, pred: 2.193751335144043\n",
      "Ep. 0/50, Ex. 2000, avg rmse: 0.6733056414446076, rolling rmse: 0.6957008600234985, immediate mse: 0.5980824828147888, target: 1.7400000095367432, pred: 0.9666420817375183\n",
      "Ep. 0/50, Ex. 2020, avg rmse: 0.6725315138483803, rolling rmse: 0.5950800478458405, immediate mse: 0.12706957757472992, target: 2.299999952316284, pred: 2.656468152999878\n",
      "Ep. 0/50, Ex. 2040, avg rmse: 0.6719992655579918, rolling rmse: 0.6182155758142471, immediate mse: 0.31861165165901184, target: 2.490000009536743, pred: 1.9255430698394775\n",
      "Ep. 0/50, Ex. 2060, avg rmse: 0.671186251606483, rolling rmse: 0.5882181778550148, immediate mse: 0.2791573703289032, target: 3.200000047683716, pred: 3.728353500366211\n",
      "Ep. 0/50, Ex. 2080, avg rmse: 0.6698969764915124, rolling rmse: 0.5370371758937835, immediate mse: 0.0005303719080984592, target: 2.009999990463257, pred: 2.033029794692993\n",
      "Ep. 0/50, Ex. 2100, avg rmse: 0.6745850930592379, rolling rmse: 1.1623836219310761, immediate mse: 3.2073287963867188, target: 0.9800000190734863, pred: 2.770901679992676\n",
      "Ep. 0/50, Ex. 2120, avg rmse: 0.6746605099554277, rolling rmse: 0.6825830549001694, immediate mse: 0.0013193481136113405, target: 1.8700000047683716, pred: 1.9063228368759155\n",
      "Ep. 0/50, Ex. 2140, avg rmse: 0.6740491609256548, rolling rmse: 0.609215596318245, immediate mse: 0.08537699282169342, target: 1.6699999570846558, pred: 1.377806544303894\n",
      "Ep. 0/50, Ex. 2160, avg rmse: 0.6745929971702778, rolling rmse: 0.7328106671571731, immediate mse: 3.23292533721542e-06, target: 1.7699999809265137, pred: 1.7682019472122192\n",
      "Ep. 0/50, Ex. 2180, avg rmse: 0.6747582993106738, rolling rmse: 0.6926191955804825, immediate mse: 3.5656626224517822, target: 2.880000114440918, pred: 0.9917038679122925\n",
      "Ep. 0/50, Ex. 2200, avg rmse: 0.6741135443397022, rolling rmse: 0.6038030147552491, immediate mse: 0.20680910348892212, target: 2.5799999237060547, pred: 2.125237226486206\n",
      "Ep. 0/50, Ex. 2220, avg rmse: 0.6753543827016842, rolling rmse: 0.8119086444377899, immediate mse: 0.00010458105680299923, target: 2.1500000953674316, pred: 2.160226583480835\n",
      "Ep. 0/50, Ex. 2240, avg rmse: 0.6763089638783536, rolling rmse: 0.7823152035474777, immediate mse: 0.4353947937488556, target: 0.7799999713897705, pred: 0.120155468583107\n",
      "Ep. 0/50, Ex. 2260, avg rmse: 0.6751061803877538, rolling rmse: 0.540334290266037, immediate mse: 0.40864184498786926, target: 3.509999990463257, pred: 2.870748996734619\n",
      "Ep. 0/50, Ex. 2280, avg rmse: 0.6753540090736411, rolling rmse: 0.7033710420131684, immediate mse: 0.9276680946350098, target: 1.440000057220459, pred: 2.4031553268432617\n",
      "Ep. 0/50, Ex. 2300, avg rmse: 0.6757670586482699, rolling rmse: 0.7228753626346588, immediate mse: 0.10476715117692947, target: 2.549999952316284, pred: 2.2263224124908447\n",
      "Ep. 0/50, Ex. 2320, avg rmse: 0.6762994727804358, rolling rmse: 0.7375537186861038, immediate mse: 0.3067835569381714, target: 3.5799999237060547, pred: 3.0261194705963135\n",
      "Ep. 0/50, Ex. 2340, avg rmse: 0.6762571827703848, rolling rmse: 0.6713494271039963, immediate mse: 0.014457020908594131, target: 2.549999952316284, pred: 2.429762601852417\n",
      "Ep. 0/50, Ex. 2360, avg rmse: 0.6773704290579361, rolling rmse: 0.8076759070158005, immediate mse: 0.22782079875469208, target: -0.20000000298023224, pred: 0.2773057520389557\n",
      "Ep. 0/50, Ex. 2380, avg rmse: 0.678804667362242, rolling rmse: 0.8481164991855621, immediate mse: 1.7503408193588257, target: 3.8299999237060547, pred: 2.506995439529419\n",
      "Ep. 0/50, Ex. 2400, avg rmse: 0.6768817446756645, rolling rmse: 0.4479577988386154, immediate mse: 0.4932069778442383, target: 2.859999895095825, pred: 2.157712936401367\n",
      "Ep. 0/50, Ex. 2420, avg rmse: 0.6768793835897481, rolling rmse: 0.6765959352254868, immediate mse: 0.007773758377879858, target: 0.8999999761581421, pred: 0.9881688952445984\n",
      "Ep. 0/50, Ex. 2440, avg rmse: 0.6780768668111942, rolling rmse: 0.8230322107672692, immediate mse: 0.17715418338775635, target: 1.7000000476837158, pred: 1.27910315990448\n",
      "Ep. 0/50, Ex. 2460, avg rmse: 0.6784961067445878, rolling rmse: 0.7296643406152725, immediate mse: 1.2994475364685059, target: 1.9900000095367432, pred: 3.1299331188201904\n",
      "Ep. 0/50, Ex. 2480, avg rmse: 0.6779232524194245, rolling rmse: 0.6074335277080536, immediate mse: 0.5777608752250671, target: 2.9600000381469727, pred: 2.1998941898345947\n",
      "Ep. 0/50, Ex. 2500, avg rmse: 0.677993960389611, rolling rmse: 0.6867652840912342, immediate mse: 1.7931549549102783, target: -1.0, pred: 0.3390873372554779\n",
      "Ep. 0/50, Ex. 2520, avg rmse: 0.6776964286758242, rolling rmse: 0.6404900878667832, immediate mse: 1.600771188735962, target: 2.2100000381469727, pred: 0.9447841644287109\n",
      "Ep. 0/50, Ex. 2540, avg rmse: 0.6779261270244032, rolling rmse: 0.7068796038627625, immediate mse: 0.0050215995870530605, target: 2.890000104904175, pred: 2.9608633518218994\n",
      "Ep. 0/50, Ex. 2560, avg rmse: 0.6778175462763191, rolling rmse: 0.6640223622322082, immediate mse: 0.5818012356758118, target: 3.700000047683716, pred: 2.9372410774230957\n",
      "Ep. 0/50, Ex. 2580, avg rmse: 0.6786937211683127, rolling rmse: 0.7908879160881043, immediate mse: 0.3419453203678131, target: 1.5499999523162842, pred: 2.134760856628418\n",
      "Ep. 0/50, Ex. 2600, avg rmse: 0.6798070375963348, rolling rmse: 0.8234805226325989, immediate mse: 1.2386023998260498, target: 3.0, pred: 1.8870748281478882\n",
      "Ep. 0/50, Ex. 2620, avg rmse: 0.6801893728032278, rolling rmse: 0.7299120664596558, immediate mse: 0.3332379460334778, target: 2.8299999237060547, pred: 3.4072675704956055\n",
      "Ep. 0/50, Ex. 2640, avg rmse: 0.6797931740587301, rolling rmse: 0.6278713285923004, immediate mse: 0.015687767416238785, target: 2.7200000286102295, pred: 2.8452508449554443\n",
      "Ep. 0/50, Ex. 2660, avg rmse: 0.6790584796736545, rolling rmse: 0.5820420861244202, immediate mse: 0.11396393179893494, target: 3.359999895095825, pred: 3.022414445877075\n",
      "Ep. 0/50, Ex. 2680, avg rmse: 0.6788225196071286, rolling rmse: 0.6474280327558517, immediate mse: 0.0014979010447859764, target: 2.109999895095825, pred: 2.0712971687316895\n",
      "Ep. 0/50, Ex. 2700, avg rmse: 0.6797873241532436, rolling rmse: 0.8091193735599518, immediate mse: 0.0699424147605896, target: 1.2000000476837158, pred: 1.4644663333892822\n",
      "Ep. 0/50, Ex. 2720, avg rmse: 0.6789725808504102, rolling rmse: 0.5689414978027344, immediate mse: 0.18557895720005035, target: 3.4000000953674316, pred: 2.9692113399505615\n",
      "Ep. 0/50, Ex. 2740, avg rmse: 0.6786433818000206, rolling rmse: 0.633855850994587, immediate mse: 0.13621419668197632, target: 1.0, pred: 1.3690720796585083\n",
      "Ep. 0/50, Ex. 2760, avg rmse: 0.6777890836553534, rolling rmse: 0.5607075229287147, immediate mse: 1.7377269268035889, target: 1.2200000286102295, pred: 2.538228750228882\n",
      "Ep. 0/50, Ex. 2780, avg rmse: 0.6778284444499476, rolling rmse: 0.6832622021436692, immediate mse: 1.579743504524231, target: 2.8399999141693115, pred: 1.583121418952942\n",
      "Ep. 0/50, Ex. 2800, avg rmse: 0.6771052706692746, rolling rmse: 0.5765479564666748, immediate mse: 0.0021999934688210487, target: 3.380000114440918, pred: 3.3330960273742676\n",
      "Ep. 0/50, Ex. 2820, avg rmse: 0.6783759960021932, rolling rmse: 0.856341078877449, immediate mse: 1.8694480657577515, target: 3.5299999713897705, pred: 2.16272234916687\n",
      "Ep. 0/50, Ex. 2840, avg rmse: 0.6782434038065226, rolling rmse: 0.6595412746071815, immediate mse: 0.86390221118927, target: 3.4200000762939453, pred: 2.490536689758301\n",
      "Ep. 0/50, Ex. 2860, avg rmse: 0.6778606272210452, rolling rmse: 0.6234872132539749, immediate mse: 0.40821900963783264, target: 1.600000023841858, pred: 0.9610798358917236\n",
      "Ep. 0/50, Ex. 2880, avg rmse: 0.6776973212170799, rolling rmse: 0.6543363973498344, immediate mse: 0.04845772683620453, target: 2.740000009536743, pred: 2.9601311683654785\n",
      "Ep. 0/50, Ex. 2900, avg rmse: 0.6777596726309889, rolling rmse: 0.6867413938045501, immediate mse: 1.372209906578064, target: 2.2200000286102295, pred: 1.048586368560791\n",
      "Ep. 0/50, Ex. 2920, avg rmse: 0.6786630838466654, rolling rmse: 0.8097028806805611, immediate mse: 1.6894513368606567, target: 3.25, pred: 1.9502110481262207\n",
      "Ep. 0/50, Ex. 2940, avg rmse: 0.6786298508224186, rolling rmse: 0.6737761676311493, immediate mse: 0.0014140409184619784, target: 1.25, pred: 1.212396264076233\n",
      "Ep. 0/50, Ex. 2960, avg rmse: 0.6781319222189385, rolling rmse: 0.604911521077156, immediate mse: 0.006967696361243725, target: 3.2799999713897705, pred: 3.1965272426605225\n",
      "Ep. 0/50, Ex. 2980, avg rmse: 0.6777724509559523, rolling rmse: 0.6245527304708958, immediate mse: 0.010242474265396595, target: 2.509999990463257, pred: 2.6112051010131836\n",
      "Ep. 0/50, Ex. 3000, avg rmse: 0.6776859339864131, rolling rmse: 0.6647905796766281, immediate mse: 0.45966941118240356, target: 2.1500000953674316, pred: 2.827989339828491\n",
      "Ep. 0/50, Ex. 3020, avg rmse: 0.6780433302948079, rolling rmse: 0.7316706463694572, immediate mse: 0.1899147480726242, target: 2.319999933242798, pred: 1.884207844734192\n",
      "Ep. 0/50, Ex. 3040, avg rmse: 0.6784719676857367, rolling rmse: 0.7432176455855369, immediate mse: 2.2071731090545654, target: 4.309999942779541, pred: 2.8243441581726074\n",
      "Ep. 0/50, Ex. 3060, avg rmse: 0.6782859710113014, rolling rmse: 0.6500051766633987, immediate mse: 0.31542158126831055, target: 2.700000047683716, pred: 3.2616240978240967\n",
      "Ep. 0/50, Ex. 3080, avg rmse: 0.6782100114999214, rolling rmse: 0.6665844082832336, immediate mse: 2.644125461578369, target: 4.300000190734863, pred: 2.6739234924316406\n",
      "Ep. 0/50, Ex. 3100, avg rmse: 0.6782082044830331, rolling rmse: 0.6779298335313797, immediate mse: 0.007475639693439007, target: 3.1700000762939453, pred: 3.083538293838501\n",
      "Ep. 0/50, Ex. 3120, avg rmse: 0.6790858764453263, rolling rmse: 0.8151689141988754, immediate mse: 0.219183087348938, target: 1.7799999713897705, pred: 2.2481698989868164\n",
      "Ep. 0/50, Ex. 3140, avg rmse: 0.6782718835252087, rolling rmse: 0.5512482883408667, immediate mse: 0.08009722828865051, target: 3.0999999046325684, pred: 2.8169853687286377\n",
      "Ep. 0/50, Ex. 3160, avg rmse: 0.678500451330227, rolling rmse: 0.7143970251083374, immediate mse: 0.905298113822937, target: 3.5999999046325684, pred: 2.6485283374786377\n",
      "Ep. 0/50, Ex. 3180, avg rmse: 0.6789476580224546, rolling rmse: 0.7496286757290364, immediate mse: 1.1030970811843872, target: 0.25, pred: 1.3002842664718628\n",
      "Ep. 0/50, Ex. 3200, avg rmse: 0.677817553080449, rolling rmse: 0.4980743620544672, immediate mse: 0.0658058300614357, target: 4.420000076293945, pred: 4.163473606109619\n",
      "Ep. 0/50, Ex. 3220, avg rmse: 0.6759982993366542, rolling rmse: 0.38482673764228825, immediate mse: 0.42437565326690674, target: 1.399999976158142, pred: 2.051441192626953\n",
      "Ep. 0/50, Ex. 3240, avg rmse: 0.675702961068055, rolling rmse: 0.6281387329101562, immediate mse: 2.913849115371704, target: 1.7000000476837158, pred: 3.4070000648498535\n",
      "Ep. 0/50, Ex. 3260, avg rmse: 0.6767288521309187, rolling rmse: 0.8429744988679886, immediate mse: 0.4858696162700653, target: 1.659999966621399, pred: 0.9629564881324768\n",
      "Ep. 0/50, Ex. 3280, avg rmse: 0.6758406862391733, rolling rmse: 0.5310252375900746, immediate mse: 2.0470547676086426, target: 1.2799999713897705, pred: 2.7107532024383545\n",
      "Ep. 0/50, Ex. 3300, avg rmse: 0.6764678086908311, rolling rmse: 0.7793472468852997, immediate mse: 0.6311470866203308, target: 3.859999895095825, pred: 3.065552234649658\n",
      "Ep. 0/50, Ex. 3320, avg rmse: 0.6768248534019065, rolling rmse: 0.7357550829648971, immediate mse: 0.4635639190673828, target: 3.5999999046325684, pred: 2.919144630432129\n",
      "Ep. 0/50, Ex. 3340, avg rmse: 0.675897893719699, rolling rmse: 0.521976238489151, immediate mse: 0.9066883325576782, target: 1.0, pred: 1.9522018432617188\n",
      "Ep. 0/50, Ex. 3360, avg rmse: 0.6768067840811528, rolling rmse: 0.8286369189620018, immediate mse: 0.9168002605438232, target: 3.8399999141693115, pred: 2.882503032684326\n",
      "Ep. 0/50, Ex. 3380, avg rmse: 0.6763678641835025, rolling rmse: 0.602607375383377, immediate mse: 0.2857533395290375, target: 2.7799999713897705, pred: 2.24544095993042\n",
      "Ep. 0/50, Ex. 3400, avg rmse: 0.6761896396699342, rolling rmse: 0.646060785651207, immediate mse: 0.44932815432548523, target: 1.4199999570846558, pred: 2.0903193950653076\n",
      "Ep. 0/50, Ex. 3420, avg rmse: 0.6763873992074538, rolling rmse: 0.7100164085626602, immediate mse: 3.203301429748535, target: 3.7699999809265137, pred: 1.9802230596542358\n",
      "Ep. 0/50, Ex. 3440, avg rmse: 0.675847498179825, rolling rmse: 0.5834974274039268, immediate mse: 0.05540326610207558, target: 3.200000047683716, pred: 2.964621067047119\n",
      "Ep. 0/50, Ex. 3460, avg rmse: 0.6758739617452724, rolling rmse: 0.6804270181804896, immediate mse: 0.016021590679883957, target: 2.5799999237060547, pred: 2.453423500061035\n",
      "Ep. 0/50, Ex. 3480, avg rmse: 0.6755213362679289, rolling rmse: 0.6144994974136353, immediate mse: 0.26682248711586, target: 2.5999999046325684, pred: 3.116548538208008\n",
      "Ep. 0/50, Ex. 3500, avg rmse: 0.6769405983418223, rolling rmse: 0.9239631623029709, immediate mse: 5.165378093719482, target: -0.07000000029802322, pred: 2.202746868133545\n",
      "Ep. 0/50, Ex. 3520, avg rmse: 0.6765694324190616, rolling rmse: 0.6115968376398087, immediate mse: 0.014799355529248714, target: 2.5999999046325684, pred: 2.7216525077819824\n",
      "Ep. 0/50, Ex. 3540, avg rmse: 0.6758157605348979, rolling rmse: 0.5431318253278732, immediate mse: 1.5119125843048096, target: 4.360000133514404, pred: 3.130401611328125\n",
      "Ep. 0/50, Ex. 3560, avg rmse: 0.6772085321458818, rolling rmse: 0.9237987458705902, immediate mse: 0.7776413559913635, target: 3.5, pred: 2.6181602478027344\n",
      "Ep. 0/50, Ex. 3580, avg rmse: 0.6754387156895859, rolling rmse: 0.36032289564609526, immediate mse: 0.15822641551494598, target: 1.0499999523162842, pred: 1.4477767944335938\n",
      "Ep. 0/50, Ex. 3600, avg rmse: 0.6750209787537231, rolling rmse: 0.600225180387497, immediate mse: 0.08402372896671295, target: 2.0999999046325684, pred: 1.8101314306259155\n",
      "Ep. 0/50, Ex. 3620, avg rmse: 0.674068672178703, rolling rmse: 0.5026058733463288, immediate mse: 0.0687585324048996, target: 2.819999933242798, pred: 2.557781457901001\n",
      "Ep. 0/50, Ex. 3640, avg rmse: 0.6742136494395453, rolling rmse: 0.700461782515049, immediate mse: 0.007498373743146658, target: 3.0, pred: 3.0865931510925293\n",
      "Ep. 0/50, Ex. 3660, avg rmse: 0.6744877517363786, rolling rmse: 0.7243880748748779, immediate mse: 0.2851414084434509, target: 3.069999933242798, pred: 2.536013603210449\n",
      "Ep. 0/50, Ex. 3680, avg rmse: 0.6746867245411836, rolling rmse: 0.7111086964607238, immediate mse: 0.37483060359954834, target: 3.180000066757202, pred: 2.567765951156616\n",
      "Ep. 0/50, Ex. 3700, avg rmse: 0.6741882261886324, rolling rmse: 0.5824396044015885, immediate mse: 0.5784874558448792, target: 2.1700000762939453, pred: 1.4094164371490479\n",
      "Ep. 0/50, Ex. 3720, avg rmse: 0.6734557976514242, rolling rmse: 0.5379198968410492, immediate mse: 0.17292918264865875, target: 2.700000047683716, pred: 3.115847587585449\n",
      "Ep. 0/50, Ex. 3740, avg rmse: 0.6736243611216088, rolling rmse: 0.7049855947494507, immediate mse: 0.08577258139848709, target: 2.640000104904175, pred: 2.9328696727752686\n",
      "Ep. 0/50, Ex. 3760, avg rmse: 0.6737551039782608, rolling rmse: 0.6982105553150177, immediate mse: 0.007217510137706995, target: 2.5, pred: 2.584955930709839\n",
      "Ep. 0/50, Ex. 3780, avg rmse: 0.6749747782301274, rolling rmse: 0.9043345212936401, immediate mse: 0.8153963088989258, target: 2.930000066757202, pred: 2.0270071029663086\n",
      "Ep. 0/50, Ex. 3800, avg rmse: 0.6748349499207321, rolling rmse: 0.6484004080295562, immediate mse: 3.2622392177581787, target: 0.8999999761581421, pred: 2.706166982650757\n",
      "Ep. 0/50, Ex. 3820, avg rmse: 0.6746778047100802, rolling rmse: 0.6448123574256898, immediate mse: 1.2957223653793335, target: 1.059999942779541, pred: 2.1982979774475098\n",
      "Ep. 0/50, Ex. 3840, avg rmse: 0.6742797902998862, rolling rmse: 0.5982391372323036, immediate mse: 0.1385287642478943, target: 3.049999952316284, pred: 2.6778054237365723\n",
      "Ep. 0/50, Ex. 3860, avg rmse: 0.6740298160501128, rolling rmse: 0.6260222613811492, immediate mse: 0.5958493947982788, target: 3.299999952316284, pred: 2.5280871391296387\n",
      "Ep. 0/50, Ex. 3880, avg rmse: 0.6735477633966261, rolling rmse: 0.5804874986410141, immediate mse: 0.04507316276431084, target: 0.36000001430511475, pred: 0.5723044276237488\n",
      "Ep. 0/50, Ex. 3900, avg rmse: 0.6731026861133854, rolling rmse: 0.5867354393005371, immediate mse: 0.9378507733345032, target: 3.799999952316284, pred: 2.831573009490967\n",
      "Ep. 0/50, Ex. 3920, avg rmse: 0.6723807466431823, rolling rmse: 0.5315664529800415, immediate mse: 0.06752079725265503, target: 2.740000009536743, pred: 2.999847650527954\n",
      "Ep. 0/50, Ex. 3940, avg rmse: 0.6727169684827544, rolling rmse: 0.7386332601308823, immediate mse: 0.010623871348798275, target: 2.990000009536743, pred: 3.0930721759796143\n",
      "Ep. 0/50, Ex. 3960, avg rmse: 0.6730828617408602, rolling rmse: 0.7451821282505989, immediate mse: 0.20959550142288208, target: -1.059999942779541, pred: -1.5178159475326538\n",
      "Ep. 0/50, Ex. 3980, avg rmse: 0.6742452213789124, rolling rmse: 0.9044505476951599, immediate mse: 0.03961803391575813, target: 2.299999952316284, pred: 2.4990427494049072\n",
      "Ep. 0/50, Ex. 4000, avg rmse: 0.6740278323148157, rolling rmse: 0.630756539106369, immediate mse: 0.002233534585684538, target: 2.8399999141693115, pred: 2.7927396297454834\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m\n\u001b[1;32m      7\u001b[0m input_representation \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m             molecule[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m      9\u001b[0m             molecule[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m             molecule[\u001b[38;5;241m6\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     15\u001b[0m             molecule[\u001b[38;5;241m7\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Making prediction\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mgnn3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_representation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Computing losses\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse_loss_fn(target, prediction)\n",
      "File \u001b[0;32m~/anaconda3/envs/mol/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/molecules/src/model.py:184\u001b[0m, in \u001b[0;36mGNN3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_message_passes):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m#if dihedral_features.size()[0] != 0:\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m#    angle_features = self.angle_dihedral_operator([torch.reshape(angle_features, [-1, 1]), torch.reshape(dihedral_features, [-1, 1]), dihedral_indices])\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     bond_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbond_angle_operator([bond_features, torch\u001b[38;5;241m.\u001b[39mreshape(angle_features, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]), angle_indices])\n\u001b[0;32m--> 184\u001b[0m     atomic_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matom_bond_operator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43matomic_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbond_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbond_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Summing the atomic features\u001b[39;00m\n\u001b[1;32m    187\u001b[0m readout_vector \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(atomic_features, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mol/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/molecules/src/model.py:147\u001b[0m, in \u001b[0;36mDMPNNLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Aggregating the messages via sum\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, node_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(node_indices):\n\u001b[0;32m--> 147\u001b[0m     messages_boxes[node_index] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m messages[i]\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Message boxes are then concatenated with the corresponding node vectors\u001b[39;00m\n\u001b[1;32m    150\u001b[0m node_message_pairs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([node_vectors, messages_boxes], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_i in range(n_epochs):\n",
    "    avg_rmse = 0\n",
    "    rolling_rmse = 0\n",
    "    rolling_avg_i = 0\n",
    "    for i, molecule in enumerate(dataset):\n",
    "        target = molecule[8].to(device)\n",
    "        input_representation = [\n",
    "                    molecule[0].to(device),\n",
    "                    molecule[1].to(device),\n",
    "                    molecule[2].to(device),\n",
    "                    molecule[3].to(device),\n",
    "                    molecule[4].to(device),\n",
    "                    molecule[5].to(device),\n",
    "                    molecule[6].to(device),\n",
    "                    molecule[7].to(device)]\n",
    "\n",
    "        # Making prediction\n",
    "        prediction = gnn3d(input_representation)\n",
    "        \n",
    "        # Computing losses\n",
    "        loss = mse_loss_fn(target, prediction)\n",
    "    \n",
    "        # Taking optimization step\n",
    "        gnn3d_optimizer.zero_grad()    \n",
    "        loss.backward()\n",
    "        gnn3d_optimizer.step()\n",
    "    \n",
    "        # Updating average losses\n",
    "        avg_rmse = (avg_rmse * i + torch.sqrt(loss).item()) / (i + 1)\n",
    "        rolling_rmse = (rolling_rmse * rolling_avg_i + torch.sqrt(loss).item()) / (rolling_avg_i + 1)\n",
    "        rolling_avg_i += 1\n",
    "    \n",
    "        if (i % printstep == 0):\n",
    "            avg_rmse_list.append(avg_rmse)\n",
    "            print(f\"Ep. {epoch_i}/{n_epochs}, Ex. {i}, avg rmse: {avg_rmse}, rolling rmse: {rolling_rmse}, immediate mse: {loss.item()}, target: {target.item()}, pred: {prediction.item()}\")\n",
    "            rolling_rmse = 0\n",
    "            rolling_avg_i = 0\n",
    "    \n",
    "    torch.save(gnn3d.state_dict(), \"./models/gnn3d_\"+str(epoch_i)+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "torch.save(gnn3d.state_dict(), \"./models/gem.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting avg RMSE List\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(avg_rmse_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 24) (3167152737.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 24\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(\"i.{i}, avg rmse: {avg_rmse}\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 24)\n"
     ]
    }
   ],
   "source": [
    "avg_rmse = 0\n",
    "for i, molecule in enumerate(dataset):\n",
    "    target = molecule[8].to(device)\n",
    "    input_representation = [\n",
    "                molecule[0].to(device),\n",
    "                molecule[1].to(device),\n",
    "                molecule[2].to(device),\n",
    "                molecule[3].to(device),\n",
    "                molecule[4].to(device),\n",
    "                molecule[5].to(device),\n",
    "                molecule[6].to(device),\n",
    "                molecule[7].to(device)]\n",
    "\n",
    "    # Making prediction\n",
    "    prediction = gnn3d(input_representation)\n",
    "    \n",
    "    # Computing losses\n",
    "    loss = mse_loss_fn(target, prediction)\n",
    "\n",
    "    # Updating average losses\n",
    "    avg_rmse = (avg_rmse * i + torch.sqrt(loss).item()) / (i + 1)\n",
    "\n",
    "    if (i % 50 == 0):\n",
    "        print(f\"i.{i}, avg rmse: {avg_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
