{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac5caa-1892-40ed-b614-08f24fee8017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Autoencoder, GNN3DFull\n",
    "from dataset import LogPDataset, LogSDataset\n",
    "import torch\n",
    "\n",
    "# Loading LogP dataset\n",
    "logp_dataset = LogPDataset(\"../data/logp\")\n",
    "print(logp_dataset)\n",
    "\n",
    "# Loading LogS dataset\n",
    "logs_dataset = LogSDataset(\"../data/logs\")\n",
    "print(logs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745ac67-50d4-4ad2-972c-e40125b0516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c26f6e5-60bf-464c-b97d-08e7adeedce5",
   "metadata": {},
   "source": [
    "# Running Tests for LogS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db787e-d880-4e5d-8eb9-ca6443c4d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_dataset_size(dataset, dataset_name, n_epochs_autoencoders = 3, printstep = 1000, min_examples = 300, divisions = 3):\n",
    "\n",
    "    atom_autoencoder = Autoencoder(80, 10).to(device)\n",
    "    bond_autoencoder = Autoencoder(10, 3).to(device)\n",
    "    \n",
    "    mse_loss_fn = torch.nn.MSELoss()\n",
    "    atom_autoencoder_optimizer = torch.optim.Adam(atom_autoencoder.parameters())\n",
    "    bond_autoencoder_optimizer = torch.optim.Adam(bond_autoencoder.parameters())\n",
    "\n",
    "    # Training autoencoders on dataset\n",
    "    atom_rmse_dict = {\"ep0\": [], \"ep1\": [], \"ep2\": []}\n",
    "    bond_rmse_dict = {\"ep0\": [], \"ep1\": [], \"ep2\": []}\n",
    "    for epoch_i in range(n_epochs_autoencoders):\n",
    "        atom_rmse = 0\n",
    "        bond_rmse = 0\n",
    "        for i, molecule in enumerate(dataset):\n",
    "            atomic_vectors = molecule[0].to(device)\n",
    "            bond_vectors = molecule[1].to(device)\n",
    "\n",
    "            atomic_vectors_reconstructed = atom_autoencoder(atomic_vectors)\n",
    "            bond_vectors_reconstructed = bond_autoencoder(bond_vectors)\n",
    "\n",
    "            atom_autoencoder_loss = mse_loss_fn(atomic_vectors, atomic_vectors_reconstructed)\n",
    "            bond_autoencoder_loss = mse_loss_fn(bond_vectors, bond_vectors_reconstructed)\n",
    "\n",
    "            # Taking optimization step\n",
    "            atom_autoencoder_optimizer.zero_grad()\n",
    "            bond_autoencoder_optimizer.zero_grad()\n",
    "            atom_autoencoder_loss.backward()\n",
    "            bond_autoencoder_loss.backward()\n",
    "            atom_autoencoder_optimizer.step()\n",
    "            bond_autoencoder_optimizer.step()\n",
    "\n",
    "            atom_rmse = (atom_rmse * i + torch.sqrt(atom_autoencoder_loss).item()) / (i + 1)\n",
    "            bond_rmse = (bond_rmse * i + torch.sqrt(bond_autoencoder_loss).item()) / (i + 1)\n",
    "\n",
    "            atom_rmse_dict[\"ep\" + str(epoch_i)].append(atom_rmse)\n",
    "            bond_rmse_dict[\"ep\" + str(epoch_i)].append(bond_rmse)\n",
    "            \n",
    "            if i % printstep == 0 or i == len(dataset) - 1:\n",
    "                print(f\"epoch.{epoch_i}, ex.{i}, atom rmse: {atom_rmse}, bond rmse: {bond_rmse}\")\n",
    "    \n",
    "    # Saving autoencoders\n",
    "    torch.save(atom_autoencoder.state_dict(), \"./models/poster/atom_autoencoder\"+ dataset_name +\".pth\")\n",
    "    torch.save(bond_autoencoder.state_dict(), \"./models/poster/bond_autoencoder\"+ dataset_name +\".pth\")\n",
    "\n",
    "    # Training GNN3D\n",
    "    \n",
    "    example_step_size = int((600 - min_examples) / divisions) + 1\n",
    "\n",
    "    gnn_rmse_dict = {}\n",
    "\n",
    "    for division_i in range(divisions + 1):\n",
    "        # Making an instance of the model and an optimizer\n",
    "        gnn3d = GNN3DFull(atomic_vector_size= 10, bond_vector_size=3, number_of_molecular_features = 200, number_of_targets = 1).to(device)\n",
    "        gnn3d_optimizer = torch.optim.Adam(gnn3d.parameters())\n",
    "        \n",
    "        avg_rmse = 0\n",
    "        gnn_rmse_dict[\"d\" + str(min_examples + example_step_size * division_i)] = []\n",
    "        \n",
    "        for i, molecule in enumerate(dataset):\n",
    "\n",
    "            if (min_examples + example_step_size * division_i < i):\n",
    "                break;\n",
    "\n",
    "            target = molecule[8].to(device)\n",
    "            input_representation = [\n",
    "                    atom_autoencoder.encode(molecule[0].to(device)),\n",
    "                    bond_autoencoder.encode(molecule[1].to(device)),\n",
    "                    molecule[2].to(device),\n",
    "                    molecule[3].to(device),\n",
    "                    molecule[4].to(device),\n",
    "                    molecule[5].to(device),\n",
    "                    molecule[6].to(device),\n",
    "                    molecule[7].to(device)]\n",
    "    \n",
    "            # Making prediction\n",
    "            prediction = gnn3d(input_representation)\n",
    "            \n",
    "            # Computing losses\n",
    "            loss = mse_loss_fn(target, prediction)\n",
    "        \n",
    "            # Taking optimization step\n",
    "            gnn3d_optimizer.zero_grad()    \n",
    "            loss.backward()\n",
    "            gnn3d_optimizer.step()\n",
    "        \n",
    "            # Updating average losses\n",
    "            avg_rmse = (avg_rmse * i + torch.sqrt(loss).item()) / (i + 1)\n",
    "            gnn_rmse_dict[\"d\" + str(min_examples + example_step_size * division_i)].append(avg_rmse)\n",
    "    \n",
    "            if (i % 20 == 0):\n",
    "                print(f\"Ep. {division_i + 1}/{divisions + 1}, Ex. {i}/{str(min_examples + example_step_size * division_i)}, avg rmse: {avg_rmse}, immediate mse: {loss.item()}, target: {target.item()}, pred: {prediction.item()}\")\n",
    "\n",
    "        #Saving model in this division\n",
    "        torch.save(gnn3d.state_dict(), \"./models/poster/gnn3d_\"+dataset_name+\"_div\"+str(min_examples + example_step_size * division_i)+\".pth\")\n",
    "    \n",
    "    return atom_rmse_dict, bond_rmse_dict, gnn_rmse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cff3c6-9e7a-4744-a804-fd7c2f4af96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_atom_rmse_dict, logs_bond_rmse_dict, logs_gnn_rmse_dict = vary_dataset_size(logs_dataset, \"logs\", min_examples = 100, divisions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef915ba-b4f0-43bc-b5d1-568f5e6513cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving dictionaries\n",
    "with open('./models/poster/loss_dicts/logs_atom_rmse_dict2.pkl', 'wb') as f:\n",
    "    pickle.dump(logs_atom_rmse_dict, f)\n",
    "\n",
    "with open('./models/poster/loss_dicts/logs_bond_rmse_dict2.pkl', 'wb') as f:\n",
    "    pickle.dump(logs_bond_rmse_dict, f)\n",
    "\n",
    "with open('./models/poster/loss_dicts/logs_gnn_rmse_dict2.pkl', 'wb') as f:\n",
    "    pickle.dump(logs_gnn_rmse_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0b42e-32d8-47d5-9b48-c71f3f68dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logp_atom_rmse_dict, logp_bond_rmse_dict, logp_gnn_rmse_dict = vary_dataset_size(logp_dataset, \"logp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8104ccad-f7ea-4d34-b395-14abaf90cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Making atom rmse plot\n",
    "for key in logp_bond_rmse_dict.keys():\n",
    "    plt.plot(logp_bond_rmse_dict[key], label = key)\n",
    "plt.legend()\n",
    "plt.title(\"Bond Autoencoder Loss for LogP\")\n",
    "plt.savefig('./models/poster/plots/bond_rmse_logp.svg', format='svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f19f1-0648-4be0-bf2f-b393cbcb17c3",
   "metadata": {},
   "source": [
    "[Download SVG Plot](./models/poster/plots/atom_rmse_logs.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff1c32-b305-4e6d-a104-8dc5037c4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making plots of gnn3d\n",
    "# Plot of each division\n",
    "logp_gnn_rmse_dict.keys()\n",
    "for i, key in enumerate(logp_gnn_rmse_dict.keys()):\n",
    "    plt.clf()\n",
    "    plt.plot(logp_gnn_rmse_dict[key], label = \"dataset size = \" + key[1:])\n",
    "    plt.legend()\n",
    "    plt.title(\"GNN3D loss in first epoch for logp\")\n",
    "    plt.savefig(\"./models/poster/plots/gnn3d_logp_\" + str(i) +\".svg\", format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4d910-2cd5-4759-8c99-afd45d5f970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sizewise plots\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "x = []\n",
    "y = []\n",
    "for key in logs_gnn_rmse_dict.keys():\n",
    "    x.append(int(key[1:]))\n",
    "    y.append(logs_gnn_rmse_dict[key][-1])\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title(\"RMSE Loss of GNN3D for LogS with increasing dataset size.\")\n",
    "plt.savefig(\"./models/poster/plots/loss_vary_dataset_logs2.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35340e0-9502-44e8-84c6-d0b998fe3b08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
