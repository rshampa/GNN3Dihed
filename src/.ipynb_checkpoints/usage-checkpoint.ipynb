{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Underlying Code\n",
    "All dataset preprocessing is done in the 'MolecularDataSet' class. To load custom data in the form of smiles and csvs one must create a base class from this class,\n",
    "which is informed about the structure of the csv file (which columns are features and which are targets).\n",
    "\n",
    "Existing derived classes exist for various dataset already (qm9, logp, logs). An example on making a custom dataset class can be found below.\n",
    "\n",
    "#### Usage\n",
    "```python\n",
    "class LogSDataset(MolecularDataSet):\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        \n",
    "        # Check if the current provided path is a folder\n",
    "        if os.path.isdir(data_path):\n",
    "            print(\"Loading dataset from folder\")\n",
    "            super().__init__(\"\", \"LogS\")\n",
    "            data_path = data_path + \"/\"\n",
    "            self.load(data_path)\n",
    "            \n",
    "        else:\n",
    "            print(\"Loading Raw Data and processing it\")\n",
    "            save_path = data_path + \"/\"            \n",
    "            data_path = data_path + \".csv\"\n",
    "            super().__init__(data_path, \"LogS\")\n",
    "            self.process()\n",
    "            # Saving the processed data\n",
    "            print(\"Saving data.\")\n",
    "            \n",
    "            # Make save path folder if it is not already there.\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            self.save(save_path)\n",
    "            \n",
    "\n",
    "        if self.data_path == \"\":\n",
    "            return\n",
    "        \n",
    "\n",
    "    def seperate_smiles_and_targets(self):\n",
    "        # Ignoring the first column which is the index\n",
    "        \n",
    "        # Loading the smiles from the first column\n",
    "        smiles = self.raw_table_data.iloc[:, 0].values\n",
    "\n",
    "        # Storing all other columns together in the form of a matrix\n",
    "        # as targets\n",
    "        targets = self.raw_table_data.iloc[:, 1:].values\n",
    "\n",
    "        return smiles, targets\n",
    "```\n",
    "\n",
    "As you can see the initializer just processes the appropriate datapath and calls relevant 'MolecularDataset' class functions. This is mostly the same for all datasets.\n",
    "The major difference occurs in the 'seperate_smiles_and_targets' functions, which tells the 'MolecularDataset' class where it can extract smiles from and where it can get all other information from.\n",
    "\n",
    "Processing data can take a considerable amount of time. For this reason, the 'MolecularDataset' has save and load functions, which can be used to quickly save and load processed data. Save and load can be manually called however, by default it is automatically called if the appropriate processed data is found at the location specified (the derived class's initializer takes care of this).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from folder\n",
      "Initializing Molecular Representation Generator\n"
     ]
    }
   ],
   "source": [
    "# Here we import a small sample of the qm9 dataset (500 molecules).\n",
    "\n",
    "from dataset import QM9\n",
    "\n",
    "qm9_sample = QM9(\"../data/smaller_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atomic Features: torch.Size([5, 80]) - This represents the atomic features of the molecule\n",
      "Bond Features: torch.Size([8, 10]) - This represents the bond features of the molecule\n",
      "Angle Features: torch.Size([12]) - This represents the angle features of the molecule\n",
      "Dihedral Features: torch.Size([0]) - This represents the dihedral features of the molecule\n",
      "Global Molecular Features: torch.Size([200]) - This represents the global molecular features of the molecule\n",
      "Bond Indices: torch.Size([8, 2]) - This represents the bond indices of the molecule\n",
      "Angle Indices: torch.Size([12, 2]) - This represents the angle indices of the molecule\n",
      "Dihedral Indices: torch.Size([0]) - This represents the dihedral indices of the molecule\n",
      "Target: torch.Size([19]) - This represents the target of the molecule\n",
      "\n",
      "\n",
      " Dataset Name: QM9\n",
      "Number of Molecules Loaded: 498\n"
     ]
    }
   ],
   "source": [
    "# To get an idea of the structure of the dataset, we can print the first molecule.\n",
    "# Moreover, the dataset itself is printable. \n",
    "\n",
    "\n",
    "molecule = qm9_sample[0]\n",
    "\n",
    "# Printing out the dimensions of all of these features with a description of what each feature is\n",
    "print(f\"Atomic Features: {molecule[0].shape} - This represents the atomic features of the molecule\")\n",
    "print(f\"Bond Features: {molecule[1].shape} - This represents the bond features of the molecule\")\n",
    "print(f\"Angle Features: {molecule[2].shape} - This represents the angle features of the molecule\")\n",
    "print(f\"Dihedral Features: {molecule[3].shape} - This represents the dihedral features of the molecule\")\n",
    "print(f\"Global Molecular Features: {molecule[4].shape} - This represents the global molecular features of the molecule\")\n",
    "print(f\"Bond Indices: {molecule[5].shape} - This represents the bond indices of the molecule\")\n",
    "print(f\"Angle Indices: {molecule[6].shape} - This represents the angle indices of the molecule\")\n",
    "print(f\"Dihedral Indices: {molecule[7].shape} - This represents the dihedral indices of the molecule\")\n",
    "print(f\"Target: {molecule[8].shape} - This represents the target of the molecule\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n\", qm9_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the architecture used for prediction here, to reduce the number of parameters in the graph neural network (to reduce overfitting and increase speed) we use autoencoders to reduce atomic and bond vector sizes before passing them in. For this however, we must first, of course train these autoencoders.\n",
    "\n",
    "A full and precise description of the implementation can be found in model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import Autoencoder # Simply importing the autoencoder model module from the model.py file\n",
    "\n",
    "# Here we create two instances of the autoencoder model, one for atoms and the other for bonds\n",
    "\n",
    "# From the earlier printed dimensions, we can see that the atomic features have a dimension of 80 while the bond features have a dimension of 10\n",
    "# We reduce these dimension sizes to 10 and 3 respectively.\n",
    "atom_autoencoder = Autoencoder(80, 10) \n",
    "bond_autoencoder = Autoencoder(10, 3)\n",
    "\n",
    "# Training is done in two phases, first the autoencoders are trained then the gnn is trained. For \n",
    "# now we begin by simply training the autoencoders.\n",
    "\n",
    "mse_loss_fn = torch.nn.MSELoss()\n",
    "atom_optimizer = torch.optim.Adam(atom_autoencoder.parameters())\n",
    "bond_optimizer = torch.optim.Adam(bond_autoencoder.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training autoencoders on logs\n",
      "LOG S Epoch: 0, ex: 0, Atom RMSE Loss: 0.28671990791939317, Bond RMSE Loss: 0.4493634861517593\n",
      "LOG S Epoch: 0, ex: 50, Atom RMSE Loss: 0.2476801213662089, Bond RMSE Loss: 0.459343467845178\n",
      "LOG S Epoch: 0, ex: 100, Atom RMSE Loss: 0.2064813139610872, Bond RMSE Loss: 0.4277830984895229\n",
      "LOG S Epoch: 0, ex: 150, Atom RMSE Loss: 0.1904817979424135, Bond RMSE Loss: 0.3641877603035393\n",
      "LOG S Epoch: 0, ex: 200, Atom RMSE Loss: 0.17994645552349764, Bond RMSE Loss: 0.3550276530088284\n",
      "LOG S Epoch: 0, ex: 250, Atom RMSE Loss: 0.16693644293082743, Bond RMSE Loss: 0.3405718693533781\n",
      "LOG S Epoch: 0, ex: 300, Atom RMSE Loss: 0.15665510029852903, Bond RMSE Loss: 0.3123839913822101\n",
      "LOG S Epoch: 0, ex: 350, Atom RMSE Loss: 0.15087960591770555, Bond RMSE Loss: 0.3009052477743147\n",
      "LOG S Epoch: 0, ex: 400, Atom RMSE Loss: 0.14413573272744484, Bond RMSE Loss: 0.28227833773886024\n",
      "LOG S Epoch: 0, ex: 450, Atom RMSE Loss: 0.1385684196418199, Bond RMSE Loss: 0.275001315346767\n"
     ]
    }
   ],
   "source": [
    "# We now write a simple training loop for the autoencoders\n",
    "\n",
    "n_epochs = 1\n",
    "printstep = 50\n",
    "\n",
    "\n",
    "print(\"Training autoencoders on logs\")\n",
    "for epoch_i in range(n_epochs):\n",
    "  avg_atom_rmse_loss = 0\n",
    "  avg_bond_rmse_loss = 0\n",
    "  for i, molecule in enumerate(qm9_sample):\n",
    "    \n",
    "    # if i > 1000:\n",
    "    #   break # Everything else is for training.\n",
    "    \n",
    "    # Putting everything onto \"device\"  \n",
    "  \n",
    "    atom_features = molecule[0]\n",
    "    bond_features = molecule[1]\n",
    "    \n",
    "    # Forward pass\n",
    "    reconstructed_atom = atom_autoencoder(atom_features)\n",
    "    reconstructed_bond = bond_autoencoder(bond_features)\n",
    "    \n",
    "    # Calculating loss\n",
    "    atom_loss = mse_loss_fn(reconstructed_atom, atom_features)\n",
    "    bond_loss = mse_loss_fn(reconstructed_bond, bond_features)\n",
    "    \n",
    "    # Backward pass\n",
    "    atom_optimizer.zero_grad()\n",
    "    bond_optimizer.zero_grad()\n",
    "    \n",
    "    atom_loss.backward()\n",
    "    bond_loss.backward()\n",
    "    \n",
    "    atom_optimizer.step()\n",
    "    bond_optimizer.step()\n",
    "    \n",
    "    # Calculating average loss\n",
    "    avg_atom_rmse_loss = (avg_atom_rmse_loss * i + atom_loss.item() ** 0.5) / (i + 1)\n",
    "    avg_bond_rmse_loss = (avg_bond_rmse_loss * i + bond_loss.item() ** 0.5) / (i + 1)\n",
    "    \n",
    "    if i % printstep == 0:\n",
    "      print(f\"LOG S Epoch: {epoch_i}, ex: {i}, Atom RMSE Loss: {avg_atom_rmse_loss}, Bond RMSE Loss: {avg_bond_rmse_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now finally move on to training the actual GNN model itself.\n",
    "# The implementation of the model can be found in model.py\n",
    "from model import GNN3D\n",
    "\n",
    "gnn3d = GNN3D(atomic_vector_size=10, bond_vector_size=3, number_of_molecular_features=200, number_of_targets=1) # We set appropriate value to the optional parameters\n",
    "\n",
    "gnn_optimizer = torch.optim.Adam(gnn3d.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 8, got 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m molecule[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m bond_autoencoder\u001b[38;5;241m.\u001b[39mencode(molecule[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mgnn3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmolecule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Calculating loss\u001b[39;00m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse_loss_fn(prediction, target)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/molecules/src/model.py:177\u001b[0m, in \u001b[0;36mGNN3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 177\u001b[0m     atomic_features, bond_features, angle_features, dihedral_features, global_molecular_features, bond_indices, angle_indices, dihedral_indices \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_message_passes):\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;66;03m#if dihedral_features.size()[0] != 0:\u001b[39;00m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;66;03m#    angle_features = self.angle_dihedral_operator([torch.reshape(angle_features, [-1, 1]), torch.reshape(dihedral_features, [-1, 1]), dihedral_indices])\u001b[39;00m\n\u001b[1;32m    183\u001b[0m         bond_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbond_angle_operator([bond_features, torch\u001b[38;5;241m.\u001b[39mreshape(angle_features, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]), angle_indices])\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 8, got 7)"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(n_epochs):\n",
    "  avg_rmse_loss = 0\n",
    "  avg_mse_loss = 0\n",
    "  for i, molecule in enumerate(qm9_sample):\n",
    "    \n",
    "    target = molecule[8]\n",
    "    molecule = list(molecule[0:7])\n",
    "    \n",
    "    \n",
    "    # Putting latent atomic and bond features through GNN3D\n",
    "    molecule[0] = atom_autoencoder.encode(molecule[0])\n",
    "    molecule[1] = bond_autoencoder.encode(molecule[1])\n",
    "      \n",
    "    # Forward pass\n",
    "    prediction = gnn3d(molecule)\n",
    "    \n",
    "    # Calculating loss\n",
    "    loss = mse_loss_fn(prediction, target)\n",
    "    \n",
    "    # Backward pass\n",
    "    gnn_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    gnn_optimizer.step()\n",
    "    \n",
    "    # Calculating average loss\n",
    "    avg_rmse_loss = (avg_rmse_loss * i + (loss.item() ** 0.5)) / (i + 1)\n",
    "    avg_mse_loss = (avg_mse_loss * i + loss.item()) / (i + 1)\n",
    "    \n",
    "    if i % printstep == 0:\n",
    "      print(f\"LOG P Epoch: {epoch_i}, ex: {i}, Avg. RMSE Loss: {avg_rmse_loss}, Avg. MSE Loss: {avg_mse_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
