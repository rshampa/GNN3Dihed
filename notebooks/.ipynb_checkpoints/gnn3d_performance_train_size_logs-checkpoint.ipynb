{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d763e732-b4ef-4ad8-abbc-c43a685fdd7f",
   "metadata": {},
   "source": [
    "## Load your libraries (LYL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c06257-a4aa-4aa0-a2cc-d61619fb23e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import dataset, molecular_representation, config, utils, model\n",
    "from dataset import QM9Dataset, LogSDataset, LogPDataset, FreeSolvDataset, ESOLDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from model import Autoencoder # Simply importing the autoencoder model module from the model.py file\n",
    "from model import GNN3D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import MolsToGridImage\n",
    "\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f15f0d0-0cb2-4642-84b0-5b63450abc49",
   "metadata": {},
   "source": [
    "# Split your dataset (SYD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd26f61-c851-43ac-8e31-06bc07555dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV\n",
    "dataset = pd.read_csv(\"../data/logs/logs.csv\")\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_dataset, test_dataset = split_dataset(dataset, 0.9)\n",
    "\n",
    "# Write train_dataset and test_dataset to CSV files\n",
    "train_dataset.to_csv(\"../data/logs/train.csv\", index=False)\n",
    "test_dataset.to_csv(\"../data/logs/test.csv\", index=False)\n",
    "\n",
    "print(\"Train and test datasets saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf0ef3-0b21-4ac2-a543-f6f230bcf33d",
   "metadata": {},
   "source": [
    "## Process your data (PYD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc2677-334d-4f86-bdcd-12f467a29fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = LogSDataset(\"../data/logs/train\")\n",
    "print(train_samples)\n",
    "print(\"===================================\")\n",
    "test_samples = LogSDataset(\"../data/logs/test\")\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5598106-3a5a-403c-ae8d-9758631af771",
   "metadata": {},
   "source": [
    "## Know your features (KYF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e120fd-0f78-40a9-b890-b237c8715434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out the dimensions of all of these features with a description of what each feature is\n",
    "print(f\"Atomic Features: {(train_samples[0])[0].shape} - This represents the atomic features of the molecule\")\n",
    "print(f\"Bond Features: {(train_samples[0])[1].shape} - This represents the bond features of the molecule\")\n",
    "print(f\"Angle Features: {(train_samples[0])[2].shape} - This represents the angle features of the molecule\")\n",
    "print(f\"Dihedral Features: {(train_samples[0])[3].shape} - This represents the dihedral features of the molecule\")\n",
    "print(f\"Global Molecular Features: {(train_samples[0])[4].shape} - This represents the global molecular features of the molecule\")\n",
    "print(f\"Bond Indices: {(train_samples[0])[5].shape} - This represents the bond indices of the molecule\")\n",
    "print(f\"Angle Indices: {(train_samples[0])[6].shape} - This represents the angle indices of the molecule\")\n",
    "print(f\"Dihedral Indices: {(train_samples[0])[7].shape} - This represents the dihedral indices of the molecule\")\n",
    "print(f\"Target: {(train_samples[0])[8].shape} - This represents the target of the molecule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b386540a-8acc-49bf-94b9-b8760824a501",
   "metadata": {},
   "source": [
    "## Know your modules (KYM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce2298-c8e1-439e-a0ef-fd5b727a0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gnn3d_vary_dataset_size(train_samples, min_samples=10, max_samples=45, divisions=3, save_dir=\"./models/vary_train_size/\", dataset_name=\"logs\"):\n",
    "    \"\"\"Train GNN3D for varying training set size\"\"\"\n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Autoencoder Training Starts\n",
    "    \"\"\"Train Autoencoder\"\"\"\n",
    "    atom_autoencoder = Autoencoder(154, 10).to(device)\n",
    "    bond_autoencoder = Autoencoder(10, 3).to(device)\n",
    "    mse_loss_fn = torch.nn.MSELoss()\n",
    "    atom_optimizer = torch.optim.Adam(atom_autoencoder.parameters())\n",
    "    bond_optimizer = torch.optim.Adam(bond_autoencoder.parameters())\n",
    "    \n",
    "    for epoch_i in range(10):\n",
    "        avg_atom_rmse_loss = 0\n",
    "        avg_bond_rmse_loss = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for i, molecule in enumerate(train_samples):\n",
    "            atom_features = molecule[0].to(device)\n",
    "            bond_features = molecule[1].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            atom_features_reconstructed = atom_autoencoder(atom_features)\n",
    "            bond_features_reconstructed = bond_autoencoder(bond_features)\n",
    "            \n",
    "            # Calculating loss\n",
    "            atom_loss = mse_loss_fn(atom_features_reconstructed, atom_features)\n",
    "            bond_loss = mse_loss_fn(bond_features_reconstructed, bond_features)\n",
    "            \n",
    "            # Backward pass and optimization step\n",
    "            atom_optimizer.zero_grad()\n",
    "            bond_optimizer.zero_grad()\n",
    "            atom_loss.backward()\n",
    "            bond_loss.backward()\n",
    "            atom_optimizer.step()\n",
    "            bond_optimizer.step()\n",
    "            \n",
    "            # Calculating average loss\n",
    "            avg_atom_rmse_loss = (avg_atom_rmse_loss * total_samples + (atom_loss.item() ** 0.5)) / (total_samples + 1)\n",
    "            avg_bond_rmse_loss = (avg_bond_rmse_loss * total_samples + (bond_loss.item() ** 0.5)) / (total_samples + 1)           \n",
    "            total_samples += 1\n",
    "    # Autoencoder Training Ends\n",
    "\n",
    "    \n",
    "    # Determine step size for each division\n",
    "    sample_step_size = int((max_samples - min_samples) / divisions) + 1\n",
    "\n",
    "    # Dictionary to store RMSE and MSE for each division\n",
    "    gnn_rmse_dict = {}\n",
    "\n",
    "    # Loop through divisions\n",
    "    for division_i in range(divisions + 1):\n",
    "        # Initialize training components for each division\n",
    "        # Aond and bond autoencoders' dimensions to be changed for a new dataset\n",
    "        atom_autoencoder = Autoencoder(154, 10).to(device)\n",
    "        bond_autoencoder = Autoencoder(10, 3).to(device)\n",
    "        mse_loss_fn = torch.nn.MSELoss()\n",
    "        gnn3d = GNN3D(atomic_vector_size=10, bond_vector_size=3, number_of_molecular_features=200, number_of_targets=1).to(device)\n",
    "        gnn_optimizer = torch.optim.Adam(gnn3d.parameters())\n",
    "\n",
    "        # Average losses initialization\n",
    "        avg_rmse = 0\n",
    "        avg_mse = 0\n",
    "\n",
    "        # Current size of the training set for this division\n",
    "        current_size = min_samples + sample_step_size * division_i\n",
    "\n",
    "        # Initialize dictionary entry for current division\n",
    "        gnn_rmse_dict[\"d\" + str(current_size)] = {\"avg_rmse\": [], \"avg_mse\": []}\n",
    "\n",
    "        # Training loop for current division\n",
    "        for epoch_i in tqdm(range(len(train_samples)), desc=f\"Division {division_i + 1}/{divisions + 1}\"):\n",
    "            if epoch_i >= current_size:\n",
    "                break\n",
    "\n",
    "            molecule = train_samples[epoch_i]\n",
    "            target = molecule[8].to(device)\n",
    "\n",
    "            input_representation = [\n",
    "                atom_autoencoder.encode(molecule[0].to(device)),\n",
    "                bond_autoencoder.encode(molecule[1].to(device)),\n",
    "                molecule[2].to(device),\n",
    "                molecule[3].to(device),\n",
    "                molecule[4].to(device),\n",
    "                molecule[5].to(device),\n",
    "                molecule[6].to(device),\n",
    "                molecule[7].to(device)\n",
    "            ]\n",
    "\n",
    "            # Forward pass\n",
    "            prediction = gnn3d(input_representation)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = mse_loss_fn(prediction, target)\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            gnn_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            gnn_optimizer.step()\n",
    "\n",
    "            # Update average RMSE and MSE\n",
    "            avg_rmse = (avg_rmse * epoch_i + torch.sqrt(loss).item()) / (epoch_i + 1)\n",
    "            avg_mse = (avg_mse * epoch_i + loss.item()) / (epoch_i + 1)\n",
    "            gnn_rmse_dict[\"d\" + str(current_size)][\"avg_rmse\"].append(avg_rmse)\n",
    "            gnn_rmse_dict[\"d\" + str(current_size)][\"avg_mse\"].append(avg_mse)\n",
    "\n",
    "            # Print progress every 10 epochs\n",
    "            if (epoch_i % 10 == 0):\n",
    "                tqdm.write(f\"Epoch: {epoch_i + 1:>4}/{current_size:>3} | Avg. RMSE Loss: {avg_rmse:.4f} | Avg. MSE Loss: {loss.item():.4f} | target: {target.item():.4f} | pred: {prediction.item():.4f}\")\n",
    "\n",
    "        # Save model state after each division\n",
    "        torch.save(gnn3d.state_dict(), f\"{save_dir}gnn3d_{dataset_name}_div{current_size}.pth\")\n",
    "\n",
    "    return gnn_rmse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f0723-eef6-483b-893a-3d04581b6aa7",
   "metadata": {},
   "source": [
    "## Know your training loss for varying dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b77e751-1f22-47ca-aec1-c407ec28b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_rmse_dict= train_gnn3d_vary_dataset_size(train_samples, min_samples=278, max_samples=1178, divisions=5, save_dir=\"./models/logs/vary_train_size/\", dataset_name=\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76961dd1-1e09-4be4-9e14-e862ac8833b2",
   "metadata": {},
   "source": [
    "## Know your plots for training loss vs. training set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aebecd8-8f62-47b6-9236-cd0471044269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it does not exist\n",
    "plot_dir = \"./models/logs/vary_train_size/plots/\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "dataset_name=\"logs\"\n",
    "\n",
    "# Plot all divisions on the same plot\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
    "\n",
    "for key in gnn_rmse_dict.keys():\n",
    "    plt.plot(gnn_rmse_dict[key][\"avg_rmse\"], label=f\"dataset size = {key[1:]}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"GNN3D RMSE Loss for Different Dataset Sizes\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"{plot_dir}gnn3d_{dataset_name}_all_divisions.svg\", format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0a952-5985-46e6-aebc-d53d664f33d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
